 - title card
    - Citation & Productivity Benefits from Open Science
    * Hi folks - I'm bill mills, and Tom Caswell invited me here today to tell you about my recent study of the open science movement, and how it could apply to research like that at Brookhaven.

 - who am i
    - bill mills
    - physicist & scientific software developer
    - triumf / cern / mozilla
    * So, who am I - I'm bill mills, physicist and scientific software developer from Vancouver. I'm currently building experimental design tools for the fine folks at TRIUMF; a number of years ago now I completed my doctoral work on the ATLAS experiment over at this little lab, and most recently I spent a year at mozilla studying this 'open science' phenomenon, which Tom asked me to come discuss today.

 - open science intro
    - what is open science?
    * who's heard of open science? Hands up.

    - There are a lot of ideas out there
    - [overwhelming number of logos]
    * There has been an absolute torrent of ideas on this topic over the past five years, trying to tackle different problems while the movement tries to find its feet, and newcomers often find it pretty overwhelming. But ultimately, open science boils down to a core set of ideas.

    - Open Science is a methodological movement that emphasizes
        - transparency
        - reuse
        - reproducibility
        - collaboration
    * Open science is a methodological movement that emphasizes these values. Anyone notice anything striking about the values of this new school? They're older than dirt! These ideas have been part of doing science pretty much since we started doing science. What open science is about is not some big sea change, but re-invigorating these old ideas with the new tools that have emerged in the last 20 years. We've done a fine job with the tools at hand for the past few centuries, but now the web, and the web really sits at the center of open science, the web has reached a level of maturity and sophistication that can supercharge how we do science, and that's what I want to dig into today.

 - practical goals
    - Deeds not words
    * OK. These are big sweeping ideas - but where does rubber hit the road here? What are we actually going to *do* to capitalize on the promise of the web - and does it actually work? For the remainder of this talk, I'm going to explore the basic things labs and individual scientists can do to push towards a more open science.

    - Four action items:
        - Open Access
        - Open Source
        - Open Data
        - Frequent Communication
    * The four values of open science that I started out with roughly map onto four actionable areas: open access, which is making sure your papers are available without a paywall; open source, which means writing code meant for reuse, and using code from others wherever possible; open data, which is basically the virtues of open access and open source combined for data; and rapid communication, which is the practice of maintaining a conversation with collaborators about your work throughout the process, rather than making huge core dumps at the end. There are also more radical things we can do and that the open science movement is chewing on, but today I want to stick to these foundational ideas.

 - Open Access
    - 1. Open Access
    * One of my favorite particle physics papers ever wasn't about the Higgs, or dark matter or supersymmetry - it was about publishing. High energy folks have been putting free preprints of their papers up on the arxiv for twenty years now - long enough, that they've now got enough data to do some statistics to it.

    - [oa-if.png]
    * The authors of this paper collected three cohorts of HEP papers: papers that were only on the arXiv; papers that were only published in traditional journals; and papers published in both places, and examined the evolution of their impact factors based on the year IF was calculated - and what they found was an enormous citation advantage for papers published in both places.
    [Source: arXiv:0906.5418](http://arxiv.org/ftp/arxiv/papers/0906/0906.5418.pdf)

    - [oa-immediate.png]
    * Digging in a bit deeper, what they found was that not only did articles published both places have a signifigant citation advantage for a decade after publishing, a big piece of their citations came before publication; the ability to consider and respond to research that is going on today generated more citations per unit time than the old news in traditional journals did at any point in their lifetime.
    [Source: arXiv:0906.5418](http://arxiv.org/ftp/arxiv/papers/0906/0906.5418.pdf)

    - [oa-greek.png]
    * In addition to the huge citation advantages that can be measured for immediate and open publishing, there's another dimension to this discussion. In summer this year, the entire Greek academic establishment abruptly lost access to the bulk of closed academic publishing when its government defaulted on its budget. And in latin america, many universities don't need to wait for a budget default; hard choices have to get made every budget on what closed journals to subscribe to, and which they can't afford. Paywalls are keeping good scientists out of the scholarly conversation and preventing them from accessing and citing our work, and helping us move the scientific conversation forward.
    [Source: Nature](http://www.nature.com/news/greek-scientists-lose-access-to-digital-journals-1.17908)

    - [oa-harvard.png]
    * It's not only international governments and institutions that are struggling with the cost burden of closed access; Harvard, one of the wealthiest academic institutions in the world, has been struggling to afford journal subscription for years, and has pushed faculty to publish in open access environments as a matter of official policy.
    [Source: The Guardian](http://www.theguardian.com/science/2012/apr/24/harvard-university-journal-publishers-prices)

    - [oa-hep-fraction.png]
    * Luckily, the tide is turning for open access. Back to my friends in HEP, by about a decade into this century, virtually all the literature in the field was finding its way onto the arXiv, for the good reasons discussed above. The precedent for open access exists, it's been present in an ad-hoc manner for years now - I'd like to see us go the final lap.
    [Source: arXiv:0906.5418](http://arxiv.org/ftp/arxiv/papers/0906/0906.5418.pdf)

    - FASTR Act
        - Fair Access to Science & Technology Research Act
        - Currently in front of the Senate
        - Mandates that the DOE release open access versions of all its papers within 6 months of publication.
    * So - with the overwhelming need and benefit to research that comes with open access, the federal government here in the US has a bill currently in front of the senate that would mandate the open-access release of papers on research done by any institution recieving more than 100M in annual federal funding. Now is the time to start thinking about how we will respond to FASTR's requirements - and how we can not just satisfy those requirements, but go even further by learning from our friends in HEP.

    - To-Do #1(a): Get Your Preprints on arXiv.
    * The next paper you publish: get its preprint on arxiv. Students and post docs, this partiularly means you! Those citations are where your future jobs are coming from, so make sure you push your PIs to get those preprints out.

    - To-Do #1(b): Get Ready for FASTR
    * To-do number 1b: at the director and PI level, it's time to start planning for FASTR. The DOE will be affected by this legislation, but the bill leaves some leeway on exactly where these manuscripts are going. The arXiv is a great candidate for our field - no need to reinvent the wheel here. Getting procedures in place that allow your labs to comply with FASTR publishing but also take advantage of the immediacy of publication benefits I described rather than waiting for the 6-month deadline mandated by FASTR is a conversation that we should start now.

<!--     - To-Do #1(b): Mandate Open Access Publishing.
    * For all the PIs and director-level folks out there, it's time to have a serious discussion about mandating open access publications for work done at our facilities. The citation benefits I illustrated not only benefit the careers of individual scientists, but raise the profile of Brookhaven as a whole. We can't just leave those advantages on the table; by baking open access into what we expect for people using our facilities, we can ensure this lab gets the visibility it deserves, and set Brookhaven up as a leader in open access science. -->

- Open Source
    - 2. Open Source
    * The second pillar of open science, and the one nearest and dearest to my heart as a scientific software developer, is open source. Open source grew out of early ARPA request-for-comments discussions in the 60s, through scripts posted on BBSes in the 80s...

    - [python][firefox][linux]
    * ...to produce some of the software that has rocked the world of computing, and touched us directly right here in the lab. Enormous wins have been achieved by projects like these, with distributed networks of individuals producing some of the most widely used and impactful software in the world. As scientists, we invest a huge amount of effort in programming; I'd like to learn from the open source movement how to fuse that effort into something as powerful as what these folks have achieved.

    - 3 Key Ingredients to an Open Source Contributor
        - Ability
        - Opportunity
        - Potential User
    * So - Zhou and Mockus found three key prerequisites for being an effective contributor on an open source project. First, there has to be tasks open that fit your skills - this is not really a problem in the scientific community, since we define the problems we're interested in, and we've all been coding long enough to be able to make progress. Second, you have to have an opportunity to contribute, which means both being aware of those tasks that fit your skills, and the time to do so; well, we're all paid to be here, so this one is also pretty trivially satisfied in the sciences. But these first two really just mean you *can* participate, but not that you will. The real secret sauce and the part that interests me most here, is this final ingredient: strong open source contributors usually see themselves as potential users of the project. 
    [Source: Zhou & Mockus](http://www.mockus.org/papers/willingness.pdf)

    - Open Source Communities Scaffold Reuse
    * Turn this around, and we can begin to understand open source communities as little incubators of software reuse. The superstar developers that helped make all those triple-A products on my first slide from this section knew that if they took their ability and opportunity and interest in usage and wrote something by themselves in a cave somewhere, you're never going to get to Python, you're never going to get to Linux solo. Incorporating other people's software is a big part of what standing on the shoulders of giants means today, and it's how diffuse, under-resourced teams can punch far above their weight, all by not reinventing wheels.

    - [os-ssi.png]
    * Back to the world of science, UK research has proven the inevitable, that software is moving to the center of the majority of research in all fields; surely there are some opportunities for reuse and collaboration there that are going untapped if we continue to write code in isolation, without sharing or discussing or collaborating on it. Instead of paying this tax on our time of hacking together code we never share and never use again, we could be investing that effort into empowering ourselves and this wider community with open, reusable software.
    [Source: Software Sustainability Institute](http://www.software.ac.uk/blog/2014-12-04-its-impossible-conduct-research-without-software-say-7-out-10-uk-researchers)

    - Many Hands Make Light Work, But Many Eyes Make Better Work
        - optimal code review turns up roughly 1 bug / 50 lines of professional code
        - bugs were cut in half (but only in half) by self-checking.
    * Beyond the efficacy of reusable code, there is a ton of research out there on the practice of code review - how many lines to review at a time and how long to spend doing it. Under optimal conditions, reviwers were finding about one bug per 50 lines of code in code written by professional software developers. Turn that around, and that sets the *minimum* rate of bugs that will be present in underviewed code - how long is your latest analysis script? You would never submit a manuscript to a journal without having your coauthors look it over first - why do we imagine that we can do better for something even more rigorous than prose? Formal code reviews in the lab might be a bit far off, but the point is that in an open source collaboration, at least we've got some eyes on our code, helping catch those mistakes.
    [Source: *Making Software: What Really Works, and Why We Believe It*, O'Rilley 2010]

    - The Discoverability Problem
    * Fine. Open source collaboration can save us time and money by helping us not reinvent the wheel, and it can improve the quality of our code by getting many sets of eyes on it. There's only one problem: what software am I working on right now? Does anyone know? Software development in the sciences is currently so diffuse, so siloed and so under-advertised, it's almost impossible to find out if and when there are interesting projects you might like to contribute to and resue in your own research. I argued above that science both can and should support an open source community, but that community will never take off unless it's easy to find out these opportunities for reuse.

    - [discoverability -->][reuse]
    * If discoverability can lead to reuse of software, but discoverability itself is difficult, is there a way to close this loop? Seems like there should be; the more a piece of software gets used, the more famous and familiar it ought to become to people in the field. Any guesses on how to make that happen?

    - [discoverability -->][reuse -->][software citation -->]
    * We can close this virtuous circle by adding software citation into our practice, which is the second big action we need to collectively take. By citing the open source software that is key to our work, we create the ability to discover relevant software just by staying on top of the literature in our fields - instead of asking the nebulous question of 'what keywords should I search on github', we can ask the question 'who is doing work similar to my own', and use the literature as our onramp to discovering software solutions.

    - [os-zenodo.png]
    * Luckily, Brookhaven and TRIUMF and many other labs are in a great position to make this happen with next to no effort, since we're already hosting a lot of our code on GitHub. GitHub and Zenodo got together recently to offer a free service to stamp software releases with DOIs - digital object identifiers, which are serial numbers corresponding to permanently archived copies of code. Simply by citing this these DOIs, we set the stage for greater discovery and reuse of scientific software.
    [Source: GitHub](https://guides.github.com/activities/citable-code/)

    - To-Do #2: DOI Your Software & Cite It!
        - GRIFFIN Collaboration *Efficiency Tracker* **1.0** 10.5281/zenodo.9887
    * So, open science to-do number 2: stamp your software with DOIs and cite 'em in your papers. Not only will this increase reuse and quality, it will help to give some recognition to the scientific software developers like me, that are helping support your research.


- Open Data
    - 3. Open Data
    * The third pillar of open science is open data. Open data can be understood as trying to take some of the wins of open access and open source, and play that same game again by publishing data sets supporting the conclusions drawn in our papers. We'd like to imagine that releasing our data will not only lead to higher visibility for our work as per open access, but also help the scientific community get more out of our data, as per open source. A fantastic study a couple of years ago on about 10k genomics papers on whether open data was actually having those effects in that field indicated that in terms of citatinos, publishing data openly gave a citation benefit of 9+-4% (95 ci).

    - [od-reuse.jpg]
    * Additionally, the same paper provided evidence suggesting that the number of new papers using old data and the amount of data getting published tracked upwards together - so this data is actually getting reused in the literature - it's not just getting warehoused on the internet and forgotten about; people actually use it. But, the question remains - how much of that usage is from a third party? 
    [ DOI: 10.7717/peerj.175/fig-3 ]

    - [od-thirdparty.jpg]
    * The same study found that this data was getting reused predominantly by thrid parties from about two years after publication.
    [ DOI: 10.7717/peerj.175/fig-4 ] 

    - More Bang For Your Buck
    * So, what this all adds up to is a small citation benefit, and a big bump in productivity per experiment - the areas under the curves in the last graph were roughly equal, meaning open data is letting us get twice as many results out of the same data - and experimentation and data collection is where a very big piece of our money goes. We've got to look for ways to squeeze every last piece of information out of those research dollars, and it seems that getting fresh pairs of eyes on those datasets gives us something like a factor of two in results. So, there is some evidence to believe that open data is actually delivering on its promises at this stage.

    - DOE Open Data Program
        - Part of Administration's Memorandum on Transparency & Open Gov't.
        - Publishes an updated Open Government Plan.
        - Maintains a public data listing of high-value data sets
    [Source: energy.gov](http://energy.gov/open-government)
    * As a result of these opportunities, institutions around the world are beginning to explore open data. The DOE is one of these institutions, and has been exploring open data through a regularly updated policy document called the Open Government Plan, and a list of high-value data sets created by the department.

    - [od-cern.png]
    * Simmilarly, the LHC experiments at CERN decided to take a swing at open data, and came up with some interesting ideas. 
    [Source: CERN Open Data Portal](http://opendata.cern.ch/?ln=en)

    - Making Open Data Work
        - Embargo periods - 5 years?
        - Data Legibility
    * If you dig into the CERN open data portal, you'll notice a few concerns implied, very politely, which actually do a pretty good job of reflecting the current discussion around open data. All the experiments at the LHC are imposing embargo periods on their data of something like five years. The other way to understand that last figure I showed, suggesting that third party reuse dominates after two years in genomics, is that first party reuse dominates before two years - people want to publish their papers on their data first, without getting scooped. Therefore, pragmatism dictates as CERN and other are concluding, that it makes sense to keep it under your hat until you've had enough time to publish your own results first. The other big concern that both CERN and I have been raising about open data, is the need to make that data legible. All these cool ideas about maximizing results with open data are going to fall spectacularly flat if those third parties can't understand the data we hand them.

    - Data Legibility
        - Standard formats (see geojson)
        - Data parsers
    * In order to ensure that others can understand their data correctly, CMS is releasing their data packaged in a VM with a great glittering edifice of analysis software to get people going - a reasonable approach for such a sophisticated experiment, but a really big central lift without much reusability - it solves the problem for CMS but really only for CMS. One thing that would be really smart to invest in before playing the full open data game, is conversations on standard data formats, like are popping up in fields from astronomy to geography to ocean science to genomics. A great example of this is geojson - the geographers standardized how they describe geographic features in code, and the result was a huge collection of geojson providers and consumers that all just fit together like a glove and generate massive amounts of data reuse, thanks to this standardization. The other benefit standardization gives you, is we can then write parsers for that data that ingest your format on disk, and pump it into memory in a class that's useful for your field, be that something ROOT-like or numpy or an R-dataframe or whatever - that way, we turn the task of consuming data into a simple piece of boilerplate, and can get right down to doing physics quickly, rather than reading data.

    - To-Do #3a: Start a discussion on a standard data format for your field.
    * IMO, this needs to happen before we start firehosing data onto the internet in a meaningful way. We need to be talking at our conferences and our collaboration meetings how we can standardize our data formats, so we can standardize our tools and analysis pipelines, and subsequently take advantage of open data.

    - To-Do #3b: Get that data out there! (and a parser, too!)
    * Once we have a resonable standard for our fields, it's time to start getting our data out there so we can maximize its productivity. And hey - if agreeing a standard data format is too tough, tidy yours up, release a parser on GitHub, and ask forgiveness later - if enough people use it, a defacto standard will arise thanks to Brookhaven.

- Open Communication
    - 4. Open Communication
    * If you like, you can actually understand open science as a communication protocol. Everything I've touched on so far has to do with communicating the various assets - papers, code and data - that make up our research. The last thing to think about is how to maximize the effectiveness of that communication. In my comments on open access, I pointed out the enormous and measurable impact immediacy has on science communication. Extrapolating to open source, a funny thing happens if you write a great glittering edifice of code in private, and then slap it up on GitHub when you're done - nobody cares. It's really hard to get people to sit down and digest 50k lines of code that just came out of left field, particularly when you are the only one talking about it.

    - Path to Healthy Collaboration:
        - Awareness
        - Participation
        - Ownership
    * In order to foster the effective collaborations that are the engine behind open science, we need to communicate about our papers, code and projects early and often in ways that invite comments. By making people aware of our projects early, it gets them thinking about them when they aren't so advanced as to be undigestable; this gets people participating early, and once they've participated for a bit, they begin to feel a sense of ownership - this project is no longer some obsucre thing that some stranger seems hell-bent on, but rather it's now their baby, too. And those are the people that are going to be the strongest advocates of your ideas and code and projects, and who will bring them back to their labs and groups and push for that collaboration on your behalf.

    - Blog Early And Blog Often
        - GitHub Issues (see the Working Open Guide)
        - Blogging (see the Journal of Brief Ideas)
        - Study Preregistration (see 'publication bias')
        - Twitter
    * There are a bunch of convenient tools out there to achieve frequent and early communication - one key thing, is that this shouldn't be hard or overly formal. Right now, scientific communication is hyper-formalized, which makes it hard to have conversations about new, maybe kind of half-baked ideas. One tool I am a huge fan of, is the issue tracker that comes with every GitHub repo (see for example [this discussion](https://github.com/mozillascience/studyGroupLessons/issues/7)). For those unfamiliar, this is just a message board that people can post questions and comments to. It's free, it comes with the repos we're already using, and it's easy to use - this is probably the most convenient option for these discussions. For some guidance on how to use issue trackers like a pro, check out the Working Open Guide from my former colleagues at Mozilla.

    Blogging is another nice way to get early ideas out there - and again, this isn't supposed to be a formal peer reviewed document - this is a quick sketch and an invitation for comments. There are a lot of simple and nice-looking blog templates for use on GitHub, but a neat and easy alternative is this thing called [The Journal of Brief Ideas](http://beta.briefideas.org/). Brief Ideas publishes articles from any field of science - the caveat being that they can be at most 200 words long. Calling Brief Ideas a 'journal' is my friend's toung in cheek joke - of course this isn't a journal, but rather a way to put an idea up there and invite comments and responses early on.

    Study Preregistration is a big movement in the life and social sciences particularly, the idea being to formally announce studies before conducting them, in order to fight the huge problems some fields are having with publication bias - literature being skewed by incentives to only publish 'positive' results. This is a bit less of a thing in particle, since no where else is seeing nothing as interesting as it is to us - setting limits on phenomena are very publishable to us.

    Finally, used well, twitter can be a useful tool for frequent communication paradigms as a distributor and aggregator of links to the content in the other three bullet points. It can be really tough to stay on top of everyone's blog, everyone's issue tracker and everything else; by pushing links to our followers every time we have a new RFC out and vice versa, we greatly simplify this process. See [this example](https://twitter.com/MozillaScience/status/628990222651428864).

    - To-Do #4: Estabish an ongoing Request For Comments discussion for your current projects.
    * So, with these tools at our disposal, it's not so tough to create the opportunity for an ongoing and transparent conversation about our research plans that include as many people as possible, and add a more casual, low-stakes dimension to scientific communication that will lead to more people talking about more ideas. I'd like to see everyone think of opportunities to seek feedback and input on their current work, and start a conversation on their issue trackers, on the journal of brief ideas, or on their own blog.

- Conclusion
    - Conclusions
    * There is very much more to the open science movement than what I've talked about so far - the four pillars of open access, open source, open data and frequent communications are the foundational elements of the movement, but as you can imagine there is a whole constelation of related ideas and nuance to dig into.

    - Four Challenges
        - Get preprints on the arXiv
        - Cite your code
        - Develop & use standardized data formats
        - Maintain an ongoing conversation about your research
    * To recap, these are the four challenges that I would like everyone to take a swing at going forward. Each of these should be in reach, and can be started by individuals or small groups, and taken together they form a great starting place to open up our research and access the citation and productivity benefits that sit behind the open science movement.

    - Thanks!










